# learn_pythonscrapy

尝试用 python 做爬虫抓取某在线资料库，并作简单自然语言处理。

### 若干文件说明

`scrapytest1`: 基础中的基础....

`scrapytest2`: 加入错误处理的 demo

`scrapytest3`: 传入 url、标签、标签属性，打印相应 `href` 的一个小方法 demo

`scrapytest4`: 采集党史资料库按年导航中的若干连接

`scrapytest5`: 从维基百科首页开始，对页面内链接进行递归跳转

`scrapytest6`: 在`scrapytest5`的基础上，采集 wikipedia 页面中的标题、段落内容及可编辑链接（如果有的话）

`scrapytest7`: 开始使用多个请求头，避开网站封锁

`scrapytest8`: 开始使用 requests 避开 403 forbidden

`scrapytest9`: 开始使用 csv 写入

`scrapytest10`: 采集包括内容、来源、时间在内的 文本

`scrapytest11`: 使用 csv 存储所有内容

`deletePunctuation`: 删除中文标点

`jieba`: 对结巴分词的简单应用
